{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SRIKAR-SILUVERI/NLP/blob/main/Lab9_WordEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import required libraries**"
      ],
      "metadata": {
        "id": "LXYyUCF4OomD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXc7xWv4POaK",
        "outputId": "213f15a6-a9a9-432e-fb29-60f58903a785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f17ypzayOGpF"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Pre-trained Word2Vec (Google News)**"
      ],
      "metadata": {
        "id": "HaCjZC6RP4lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load pre-trained Word2Vec model (may take time on first download)\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Print vocabulary size\n",
        "print(\"Vocabulary Size:\", len(model.key_to_index))\n",
        "\n",
        "# Display vector for a sample word\n",
        "word = \"apple\"\n",
        "vector = model[word]\n",
        "\n",
        "print(\"\\nWord:\", word)\n",
        "print(\"Vector length:\", len(vector))\n",
        "print(\"First 10 values of the vector:\\n\", vector[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5L7euoPPAJL",
        "outputId": "f7535ab0-5bab-44ca-9094-9983bb0e0656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 3000000\n",
            "\n",
            "Word: apple\n",
            "Vector length: 300\n",
            "First 10 values of the vector:\n",
            " [-0.06445312 -0.16015625 -0.01208496  0.13476562 -0.22949219  0.16210938\n",
            "  0.3046875  -0.1796875  -0.12109375  0.25390625]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Pre-trained GloVe**"
      ],
      "metadata": {
        "id": "Sop_aq4lRexj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load GloVe embeddings (100-dimensional)\n",
        "model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Print vocabulary size\n",
        "print(\"Vocabulary Size:\", len(model.key_to_index))\n",
        "\n",
        "# Display vector for a sample word\n",
        "word = \"machine\"\n",
        "vector = model[word]\n",
        "\n",
        "print(\"\\nWord:\", word)\n",
        "print(\"Vector length:\", len(vector))\n",
        "print(\"First 10 values of the vector:\\n\", vector[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzL0VxTcP965",
        "outputId": "3d61a557-18ec-4e1f-b48c-52ed5bc8d53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 400000\n",
            "\n",
            "Word: machine\n",
            "Vector length: 100\n",
            "First 10 values of the vector:\n",
            " [-0.65365  0.49419 -0.26245 -0.20722 -0.11413  0.35701  1.0454   0.21881\n",
            "  0.52769  0.60606]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explore Word Similarity**"
      ],
      "metadata": {
        "id": "tBzOclsNR7dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe model (100D)\n",
        "model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Define word pairs\n",
        "word_pairs = [\n",
        "    ('computer','laptop'),\n",
        "    ('mobile','phone'),\n",
        "    ('city','village'),\n",
        "    ('engineer','scientist'),\n",
        "    ('hospital','clinic'),\n",
        "    ('school','university'),\n",
        "    ('car','road'),\n",
        "    ('music','song'),\n",
        "    ('rain','cloud'),\n",
        "    ('earth','planet'),\n",
        "    ('teacher','professor'),\n",
        "    ('india','china'),\n",
        "    ('football','cricket'),\n",
        "    ('river','lake'),\n",
        "    ('happy','joy'),\n",
        "    ('sad','cry'),\n",
        "    ('bird','airplane'),\n",
        "    ('mountain','hill'),\n",
        "    ('book','library'),\n",
        "    ('market','shop')\n",
        "]\n",
        "\n",
        "print(\"Word Similarity Scores:\\n\")\n",
        "\n",
        "for w1, w2 in word_pairs:\n",
        "    similarity = model.similarity(w1, w2)\n",
        "    print(f\"{w1} - {w2} : {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIy5Fp9-R8Ya",
        "outputId": "69874e60-a1f0-4dde-b73c-e519608ee5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Similarity Scores:\n",
            "\n",
            "computer - laptop : 0.7024\n",
            "mobile - phone : 0.7307\n",
            "city - village : 0.6327\n",
            "engineer - scientist : 0.6081\n",
            "hospital - clinic : 0.8140\n",
            "school - university : 0.7548\n",
            "car - road : 0.5374\n",
            "music - song : 0.7319\n",
            "rain - cloud : 0.5149\n",
            "earth - planet : 0.8551\n",
            "teacher - professor : 0.6188\n",
            "india - china : 0.5997\n",
            "football - cricket : 0.6660\n",
            "river - lake : 0.7426\n",
            "happy - joy : 0.5189\n",
            "sad - cry : 0.5766\n",
            "bird - airplane : 0.2634\n",
            "mountain - hill : 0.6858\n",
            "book - library : 0.5616\n",
            "market - shop : 0.4741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nearest Neighbor Exploration**"
      ],
      "metadata": {
        "id": "SlBbJZWNSOHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe embeddings (100D)\n",
        "model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Choose at least 5 words, converting them to lowercase for better vocabulary match\n",
        "chosen_words = [\"india\", \"computer\", \"nlp\", \"machine\", \"ai\"]\n",
        "\n",
        "for word in chosen_words:\n",
        "    print(f\"\\nTop similar words for '{word}':\\n\")\n",
        "\n",
        "    similar_words = model.most_similar(word, topn=5)\n",
        "\n",
        "    for similar_word, score in similar_words:\n",
        "        print(f\"{similar_word} : {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4pVk2H3SHHJ",
        "outputId": "e5c6d5e0-7c31-4331-d823-91d9cdd7c239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top similar words for 'india':\n",
            "\n",
            "pakistan : 0.8370\n",
            "indian : 0.7802\n",
            "delhi : 0.7712\n",
            "bangladesh : 0.7662\n",
            "lanka : 0.7639\n",
            "\n",
            "Top similar words for 'computer':\n",
            "\n",
            "computers : 0.8752\n",
            "software : 0.8373\n",
            "technology : 0.7642\n",
            "pc : 0.7366\n",
            "hardware : 0.7290\n",
            "\n",
            "Top similar words for 'nlp':\n",
            "\n",
            "hagelin : 0.6065\n",
            "oth : 0.5276\n",
            "grn : 0.5221\n",
            "lib : 0.5205\n",
            "inp : 0.4841\n",
            "\n",
            "Top similar words for 'machine':\n",
            "\n",
            "machines : 0.7854\n",
            "device : 0.6773\n",
            "equipment : 0.6412\n",
            "gun : 0.6409\n",
            "guns : 0.6362\n",
            "\n",
            "Top similar words for 'ai':\n",
            "\n",
            "hey : 0.6296\n",
            "sugiyama : 0.6117\n",
            "gonna : 0.5951\n",
            "ya : 0.5950\n",
            "fukuhara : 0.5866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Analogy Tasks**"
      ],
      "metadata": {
        "id": "ALiN8_MiTTTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained Word2Vec (better for analogies)\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Analogy 1\n",
        "result1 = model.most_similar(\n",
        "    positive=[\"doctor\", \"school\"],\n",
        "    negative=[\"hospital\"],\n",
        "    topn=5\n",
        ")\n",
        "\n",
        "# Analogy 2\n",
        "result2 = model.most_similar(\n",
        "    positive=[\"bigger\", \"small\"],\n",
        "    negative=[\"big\"],\n",
        "    topn=5\n",
        ")\n",
        "\n",
        "# Analogy 3\n",
        "result3 = model.most_similar(\n",
        "    positive=[\"mother\", \"man\"],\n",
        "    negative=[\"woman\"],\n",
        "    topn=5\n",
        ")\n",
        "\n",
        "\n",
        "print(result1)\n",
        "\n",
        "\n",
        "print(result2)\n",
        "\n",
        "\n",
        "print(result3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-I_vIUnSxGI",
        "outputId": "5b40f2d0-8846-4e6f-b141-83bd79a72475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('guidance_counselor', 0.5969594717025757), ('teacher', 0.5755364298820496), ('eighth_grade', 0.5226408243179321), ('schoolers', 0.5168290138244629), ('elementary', 0.5085657238960266)]\n",
            "[('larger', 0.7402471899986267), ('smaller', 0.7329993844032288), ('tiny', 0.5698219537734985), ('tinier', 0.543969452381134), ('large', 0.5191665887832642)]\n",
            "[('father', 0.8097569346427917), ('son', 0.7835153937339783), ('uncle', 0.7502947449684143), ('dad', 0.7315300703048706), ('brother', 0.7288522124290466)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word2Vec Similarity**"
      ],
      "metadata": {
        "id": "bM9GYRvRnCbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load Word2Vec\n",
        "word2vec = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "pairs = [\n",
        "    ('doctor','nurse'),\n",
        "    ('cat','dog'),\n",
        "    ('car','bus'),\n",
        "    ('king','queen'),\n",
        "    ('teacher','student'),\n",
        "    ('man','woman'),\n",
        "    ('apple','orange'),\n",
        "    ('university','college'),\n",
        "    ('river','water'),\n",
        "    ('sun','moon')\n",
        "]\n",
        "\n",
        "print(\"Word2Vec Similarity Scores:\\n\")\n",
        "\n",
        "for w1, w2 in pairs:\n",
        "    print(w1, \"-\", w2, \":\", round(word2vec.similarity(w1,w2),4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDQmr-qhjtkF",
        "outputId": "f17e18e7-5fb9-4438-a35e-3c18312d3fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Similarity Scores:\n",
            "\n",
            "doctor - nurse : 0.632\n",
            "cat - dog : 0.7609\n",
            "car - bus : 0.4693\n",
            "king - queen : 0.6511\n",
            "teacher - student : 0.6301\n",
            "man - woman : 0.7664\n",
            "apple - orange : 0.392\n",
            "university - college : 0.6385\n",
            "river - water : 0.5769\n",
            "sun - moon : 0.4263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word2Vec Neighbours**"
      ],
      "metadata": {
        "id": "trVvugGVnGgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"king\", \"doctor\", \"car\", \"music\", \"university\"]\n",
        "\n",
        "print(\"Word2Vec Nearest Neighbours:\\n\")\n",
        "\n",
        "for word in words:\n",
        "    print(f\"\\nTop similar words for '{word}':\\n\")\n",
        "    similar_words = word2vec.most_similar(word, topn=5)\n",
        "    for sim_word, score in similar_words:\n",
        "        print(sim_word, \":\", round(score,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAC4KQe9jw_0",
        "outputId": "028d62a5-520c-43d0-8123-e4fe57be59d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Nearest Neighbours:\n",
            "\n",
            "\n",
            "Top similar words for 'king':\n",
            "\n",
            "kings : 0.7138\n",
            "queen : 0.6511\n",
            "monarch : 0.6413\n",
            "crown_prince : 0.6204\n",
            "prince : 0.616\n",
            "\n",
            "Top similar words for 'doctor':\n",
            "\n",
            "physician : 0.7806\n",
            "doctors : 0.7477\n",
            "gynecologist : 0.6948\n",
            "surgeon : 0.6793\n",
            "dentist : 0.6785\n",
            "\n",
            "Top similar words for 'car':\n",
            "\n",
            "vehicle : 0.7821\n",
            "cars : 0.7424\n",
            "SUV : 0.7161\n",
            "minivan : 0.6907\n",
            "truck : 0.6736\n",
            "\n",
            "Top similar words for 'music':\n",
            "\n",
            "classical_music : 0.7198\n",
            "jazz : 0.6835\n",
            "Music : 0.6596\n",
            "Without_Donny_Kirshner : 0.6416\n",
            "songs : 0.6396\n",
            "\n",
            "Top similar words for 'university':\n",
            "\n",
            "universities : 0.7004\n",
            "faculty : 0.6781\n",
            "unversity : 0.6758\n",
            "undergraduate : 0.6587\n",
            "univeristy : 0.6585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec Analogy**"
      ],
      "metadata": {
        "id": "dJltnBgxnK-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word2Vec Analogy Results:\\n\")\n",
        "\n",
        "print(\"king - man + woman =\")\n",
        "print(word2vec.most_similar(positive=['king','woman'], negative=['man'], topn=5))\n",
        "\n",
        "print(\"\\nparis - france + india =\")\n",
        "print(word2vec.most_similar(positive=['paris','india'], negative=['france'], topn=5))\n",
        "\n",
        "print(\"\\nteacher - school + hospital =\")\n",
        "print(word2vec.most_similar(positive=['teacher','hospital'], negative=['school'], topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ps79nqekdiI",
        "outputId": "aecf1c62-d10e-44ae-d9fd-643e821305b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Analogy Results:\n",
            "\n",
            "king - man + woman =\n",
            "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581)]\n",
            "\n",
            "paris - france + india =\n",
            "[('chennai', 0.5442505478858948), ('delhi', 0.5149926543235779), ('mumbai', 0.5024341344833374), ('hyderabad', 0.49932485818862915), ('gujarat', 0.48732805252075195)]\n",
            "\n",
            "teacher - school + hospital =\n",
            "[('Hospital', 0.6331106424331665), ('nurse', 0.6280134320259094), ('hopsital', 0.6217317581176758), ('intensive_care', 0.5683753490447998), ('Hosptial', 0.5647749304771423)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GloVe Analogy**"
      ],
      "metadata": {
        "id": "Q_iYj-jynOhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe\n",
        "glove = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "print(\"GloVe Analogy Results:\\n\")\n",
        "\n",
        "print(\"king - man + woman =\")\n",
        "print(glove.most_similar(positive=['king','woman'], negative=['man'], topn=5))\n",
        "\n",
        "print(\"\\nparis - france + india =\")\n",
        "print(glove.most_similar(positive=['paris','india'], negative=['france'], topn=5))\n",
        "\n",
        "print(\"\\nteacher - school + hospital =\")\n",
        "print(glove.most_similar(positive=['teacher','hospital'], negative=['school'], topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwrPMdHrkgU8",
        "outputId": "56ffc2ae-34ce-49af-97f2-26909b361270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe Analogy Results:\n",
            "\n",
            "king - man + woman =\n",
            "[('queen', 0.7698540687561035), ('monarch', 0.6843381524085999), ('throne', 0.6755736470222473), ('daughter', 0.6594556570053101), ('princess', 0.6520534157752991)]\n",
            "\n",
            "paris - france + india =\n",
            "[('delhi', 0.8654932975769043), ('mumbai', 0.7718895077705383), ('bombay', 0.7222235798835754), ('dhaka', 0.6891742944717407), ('calcutta', 0.6761991381645203)]\n",
            "\n",
            "teacher - school + hospital =\n",
            "[('nurse', 0.7798740267753601), ('doctor', 0.76134192943573), ('patient', 0.6908571124076843), ('physician', 0.6851393580436707), ('hospitalized', 0.6718847751617432)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 8 :Reflection and Interpretation**"
      ],
      "metadata": {
        "id": "CBocn_WflrNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, I learned how Word2Vec and GloVe convert words into numerical vectors. These vectors help the model understand word meanings. Similar words like doctor and nurse showed high similarity scores. The nearest neighbour task showed that related words appear close in vector space. In the analogy task, king – man + woman gave queen, which shows the model understands relationships. Word2Vec performed slightly better in analogy results"
      ],
      "metadata": {
        "id": "1G1A46xUluhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 9 — Lab Report**"
      ],
      "metadata": {
        "id": "KuRWvwicl4am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Title**\n",
        "\n",
        "Word2Vec and GloVe Word Embedding Analysis\n",
        "\n",
        "**Objective**\n",
        "\n",
        "To understand and analyze Word2Vec and GloVe models using similarity, neighbours, and analogy tasks.\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "Word embeddings represent words as numerical vectors. Similar words are placed close together in vector space.\n",
        "\n",
        "**Model Description**\n",
        "\n",
        "Word2Vec uses neural networks to learn word relationships from context.\n",
        "GloVe uses global word co-occurrence statistics to generate vectors.\n",
        "\n",
        "**Results**\n",
        "\n",
        "Similarity scores were high for related words.\n",
        "Nearest neighbour results showed meaningful similar words.\n",
        "Analogy tasks like king – man + woman gave queen.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Both models capture semantic relationships effectively. Word2Vec performed slightly better in analogy tasks."
      ],
      "metadata": {
        "id": "mnVGq04DmAhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7 Theory Answers (Very Short)**\n",
        "\n",
        "**Word Embedding:** Representation of words as numerical vectors.\n",
        "\n",
        "**Word2Vec:** Neural network model to generate word vectors.\n",
        "\n",
        "**CBOW:** Predicts word from context.\n",
        "\n",
        "**Skip-Gram:**Predicts context from word.\n",
        "\n",
        "**GloVe:** Uses global word co-occurrence statistics.\n",
        "\n",
        "**Cosine Similarity:**Measures similarity between two vectors.\n",
        "\n",
        "**Application:** Used in chatbots, translation, sentiment analysis."
      ],
      "metadata": {
        "id": "H0hTjUv1mS8p"
      }
    }
  ]
}